---
title: 'About MARL'
description: 'About MARL'
pubDate: 'October 25 2025'
heroImage: '../../assets/marl.png'
---

Image source: https://www.researchgate.net/figure/Multi-agent-deep-reinforcement-learning-diagram-where-the-agents-are-allowed-to-exchange_fig1_333971638.

If you got here from my LinkedIn post, then you already know what I'm gonna talk about. But if that wasn't the case, then you can sorta tell from the thumbnail that this won't be pretty.

MARL stands for Multi-Agent Reinforcement Learning. It's a sub-field of reinforcement learning. You might have learned reinforcement learning in one of your undergraduate CS courses, but you probably haven't learned MARL. And to clarify, I'm a CS major with no specializations. And if anything, I knew more about LLMs before I got here since that shit was always making the news. I did, however, take a course that went through reinforcement learning. It's CS486 at the University of Waterloo if you're curious about it. So I do know what the heck reinforcement learning is.

What about MARL then?

Well, it was a coincidence. One day, I happened to stumble upon a mini-course in MARL offered by my previous university in Vietnam, one where I studied at for 4 months before moving to Waterloo due to visa shenanigans (thanks IRCC). And the lectures were offered both in-person and online via Zoom. And because it was in Vietnam, lectures starting at 8:30AM there meant 9:30PM for me. Perfect timing for me. So, since it didn't hurt to sign up and listen, I did just that.

The lectures and slides were based on [this particular book](https://www.marl-book.com/). So you can just read the book and the slides there if you wanna. But I'm not known for having a good attention span when reading books. So I listened to the lectures anyway. After the mini-course concluded, I gained a pretty good idea or two about MARL. And I thought I wanna share it to other people who may have interests. That being said, I'm not gonna fill this post with math formulas. This is just me trying to explain stuff on an intuitive level.

So, let's get started.

#### 1. What the heck is even reinforcement learning?

Reinforcement learning is a methodology used to solve a very particular class of problems. The simplest problem involves an agent and an environment. An agent is basically anything capable of interacting with its environment. Think of it as a shipper running on the streets doing their job. An environment is well, an environment. It has our agent, paths, it has obstacles, it has people walking around, it has whatever. For this simplest example, we assume our sole agent is the only thing in the environment that is capable of interacting with the environment. Now, an agent has goals that it needs to reach. Think of it as the shipper doing the delivery. They have two goals: deliver the parcel to the right location, and deliver it as quick as possible. 

Initially, the agent doesn't know a damn thing about its environment. So, how can it reach its goals? First, you define a set of states that the agent can be in at any given time and a set of actions that the agent can take at any given time. Then, you make a mapping. If the agent is currently in state S and they take action A, they land in state S' and get a reward R. The reward tells the agent whether they did a good job making that move just now, or whether they screwed up royally. If a shipper turns to a street that does lead to the destination, then the reward value is high. If they instead turn to a dead end, then the reward value is low. The goals of the agent determines how the rewards are laid out. In other words, we have a reward function. The endgame here is to have the agent learn the optimal policy, i.e. given a fixed reward function, if the agent is currently in state S, it should take the action that will benefit it the most in the journey towards the goals. There are a bunch of math formulas that tell the agent how to evaluate whether its move was amazing or ass, and tell it how to update its current policy. You train the agent over many iterations and it will eventually smarten up.

So that's the basics of reinforcement learning. If you wanna imagine how much more there is to reinforcement learning, do me a favor and remember that it took you 2 minutes to read up until this point, yet 2 months for me to learn reinforcement learning in CS486.

#### 2. So what exactly is MARL?

Now we have a new problem. In reality, our agent isn't the only thing capable of interacting with its environment. There are other agents as well, each with its own set of goals and policies. So now, our agent somehows needs to be able to reach its goals, while taking other agents' policies into account to learn its own optimal policy. And each and every other agent is also doing the same thing as our agent! Think of it as a game of chess, where both you and your opponent continuously learn and adjust your strategies based on the actions taken by the other person. Except, unfortunately, you don't always have just two agents. As such, we dedicate an entire sub-field to studying this problem. Multi-Agent Reinforcement Learning.

#### 3. So how do we do MARL?

Initially, the computer scientists started out with a very simple idea. 

"Why don't we just let each agent learn its own optimal policy and not give a shit about any other agent?"

Turns out, that wasn't a very good idea. Some agents get what they want, while others veer off the track completely. Which is not ideal, since we do want to have as many agents reaching their goals as possible. Turns out, we need to somehow translate the concept of cooperation into math formulas.

Second idea. We have a function Q that incorporates all agents, we go through all possible combinations of actions that these agents could take, and we update that massive function Q. Theoretically speaking, since each action of each agent influences the Q-function, we have obtained cooperation, problem solved! Practically speaking, that's exponential blowup. Very very ugly.

Third idea. We introduce game theory and the concept of equilibriums into the algorithm. I'm not gonna explain what the heck game theory is, because that's another whole ass course. But the idea is, instead of one function brute-forcing through every single combination of actions, at the current state, you consider the state of all agents, you use game theory tools to compute the optimal next state that would benefit all agents, and you update each agent accordingly. While the previous idea led to exponential blowup in computation, the computation here scales **linearly** with the number of agents. Which is very very nice. There are a lot of different approaches to either calculate or approximate this optimal next state, all of which you can learn more from the book.

#### 4. So are we done?

Well, no. What we've been talking up until this point makes one core assumption: each agent has a different set of goals. But what if the agents are working towards a common goal instead? That is, what if cooperation is now the core component of our learning, instead of a nice thing to have like previously? Sure, the game theory thingimajig still works, but that one was specifically designed for different goals. So now, if we have a common goal, can we do something better? The answer is, of course, yes. Otherwise I wouldn't be writing a new section for this. Also, we now go back to the Q-function since cooperation just got more interesting.

First idea. Decomposition of Q-function. We know that this Q-function incorporates all agents. But exponential blowup threw it out of the window. So, since each agent influences this Q-function, we give each agent its own Q-function, and try to determine what sort of combination of these individual Q-functions would give us the overall Q-function. First sub-idea is linear combination. Yeah well, I don't know why that was even reasonable to begin with. But it was there in the history. Second sub-idea is non-linear monotonic combination. We use a bunch of neural networks to learn the combination, which gives us the non-linear freedom, but we constrain the networks to one condition: if an agent takes an action that is optimal for itself, that action is also optimal for the whole team, or monotonicity for short. Third sub-idea, we ditch the monotonic thing completely, which gives us the best solution, theoretically speaking. Unfortunately, in practice, it's unstable and hard to optimize.

Second idea. Critic-actor networks. You have a neural network that oversees the global state of all agents (the critic), and a neural network for each and every agent (the actors). It still has the same cooperation spirit, but now, since there is a critic, the whole system can learn which agent actually contributed to the overall progress of the team, which makes it more efficient to train, since you know which agents you should focus on more.

Third idea. Communication. "Wait how come we don't have communication until now?" Don't ask me. Anyways, the idea is we let each agent communicate to other agents, meaning we no longer need a supervisor. It can either be agent-to-agent (think of private chat), or a global message state (think of group chat). You ever heard of collective intelligence? That's exactly what this is.

Fourth idea. Role-base communication. Maybe not all agents need to communicate back and forth. Maybe some agents need only to send messages. Maybe some agents need only to read messages. You let each agent gradually learn what their role is, which can make it more efficient than the previous idea.

#### 5. So are we done now?

I'm pretty sure there's **a lot** more I didn't cover. If you want to learn more, there's always the book. The key thing to remember here, is that there is no such thing as a paddy's cure. Each MARL algorithm has its specific use cases where it shines the most. So, the one thing to do before picking an approach is to know exactly what your problem needs.

There's also this note about LLMs that I took during a small conference I attended. The conference discussed about math and economics, and sure enough, the question about the impact of LLMs came up. I brought this conference up because you see, what is economics, if not a bunch of people trying to see what others are doing and coming up with an optimal strategy? Precisely the kind of problem that MARL can help with. An issue with economics research is that you do need actual people participating in a simulation to study the effects, and getting a good sample of people isn't always so straightforward. LLMs offer a promising potential to resolve this issue. We could have LLMs as the agents in our MARL problem and run many simulations with relative ease. In fact, I'm pretty sure researchers have already explored or are exploring this idea. But yeah, this is one of the few cases where LLMs are actually crucially useful.

And with that, this post is now concluded. Go read the book.